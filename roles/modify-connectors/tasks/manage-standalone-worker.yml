# (c) 2016 DataNexus Inc.  All Rights Reserved
---
# if a connector worker is not already running at the named URL, then
# start a new connector worker in distributed mode
- name: Set some facts we'll use later
  set_fact:
    kfka_nodes: "{{kafka_nodes | map('extract', hostvars, [('ansible_' + api_iface), 'ipv4', 'address']) | list}}"
    worker_props_filename: "{{(worker_name != '') | ternary('worker-' + worker_name + '.properties', 'worker.properties')}}"
    worker_offset_filename: "{{(worker_name != '') | ternary('worker-' + worker_name + '.offsets', 'worker.offsets')}}"
    worker_grep_str: "{{(worker_name != '') | ternary(' | grep ' + worker_name, '')}}"
    kafka_topics_cmd: "{{(kafka_distro == 'apache') | ternary(kafka_dir + '/bin/kafka-topics.sh', 'kafka-topics')}}"
    zk_node_str: "{{(zk_nodes | default(['localhost'])) | join(':2181,')}}:2181"
    topic_repl_factor: "{{[3,(kafka_nodes | length)] | min}}"
- name: Check to see if a worker is already running
  uri:
    method: GET
    url: "{{connector_worker_url}}"
    return_content: yes
  register: returned_content
  when: action == 'start-standalone-workers'
  failed_when: false
# if a worker is not running at the named URL and we've been asked to start
# a connector worker, then start it
- block:
  # first, construct a properties file from the worker_properties hash and
  # the cluster configuration
  - name: Ensure directory for worker properties file exists
    file:
      path: "/etc/kafka-connectors"
      state: directory
      owner: "{{kafka_user}}"
      group: "{{kafka_group}}"
      mode: 0755
  - name: Create {{worker_props_filename}} file
    template:
      src: standalone-worker.properties.j2
      dest: "/etc/kafka-connectors/{{worker_props_filename}}"
      owner: "{{kafka_user}}"
      group: "{{kafka_group}}"
      mode: 0644
  # if additional configuration parameters were passed in, then set the corresponding
  # parameters in the worker properties file
  - name: Setup additional configuration options
    lineinfile:
      dest: "/etc/kafka-connectors/{{worker_props_filename}}"
      line: "{{config_options.name}}={{config_options.value}}"
      insertafter: "{{config_options.insertafter}}"
      state: present
    with_items:
      # set flag to enable converter schemas?
      - name: "key.converter.schemas.enable"
        value: "{{key_converter_schemas_enable}}"
        nil_val: "true"
        insertafter: "^key.converter="
      - name: "value.converter.schemas.enable"
        value: "{{value_converter_schemas_enable}}"
        nil_val: "true"
        insertafter: "^value.converter="
      # set schema registry URL?
      - name: "key.converter.schema.registry.url"
        value: "{{key_converter_schema_registry_url}}"
        nil_val: ""
        insertafter: "^key.converter="
      - name: "value.converter.schema.registry.url"
        value: "{{value_converter_schema_registry_url}}"
        nil_val: ""
        insertafter: "^value.converter="
      # set flag to enable internal converter schemas?
      - name: "internal.key.converter.schemas.enable"
        value: "{{internal_key_converter_schemas_enable}}"
        nil_val: "true"
        insertafter: "^internal.key.converter="
      - name: "internal.value.converter.schemas.enable"
        value: "{{internal_value_converter_schemas_enable}}"
        nil_val: "true"
        insertafter: "^internal.value.converter="
      # set internal schema registry URL?
      - name: "internal.key.converter.schema.registry.url"
        value: "{{internal_key_converter_schema_registry_url}}"
        nil_val: ""
        insertafter: "^internal.key.converter="
      - name: "internal.value.converter.schema.registry.url"
        value: "{{internal_value_converter_schema_registry_url}}"
        nil_val: ""
        insertafter: "^internal.value.converter="
    loop_control:
      loop_var: config_options
    when: config_options.value != config_options.nil_val
  # and create the properties files for the connectors in the connector_list
  - name: Initialize list of connector properties files
    set_fact:
      connector_prop_files: []
  - name: Create new connector properties files
    include_tasks: create-standalone-connector-config.yml
    vars:
      connector: "{{connector_item}}"
    with_items: "{{connector_list}}"
    loop_control:
      loop_var: connector_item
  # then, use that '{{worker_props_filename}}' file to start the connector worker via
  # the 'connect-distributed' command in 'daemon mode'
  - name: Start worker
    shell: "connect-standalone -daemon /etc/kafka-connectors/{{worker_props_filename}} {{connector_prop_files | join(' ')}}"
    become_user: "{{kafka_user}}"
  become: true
  when:
    - action == 'start-standalone-workers'
    - "'Connection refused' in returned_content.msg"
    - returned_content.status == -1
# if a worker is running at the named URL and we've been asked to stop
# the worker then kill the daemon process
- block:
    # get the process ID of the daemon
    - name: Get process ID for daemon
      shell:  "ps aux | grep org.apache.kafka.connect.cli.ConnectStandalone{{worker_grep_str}} | grep -v grep | awk '{print $2}'"
      register: ps_output
    # and kill that process ID
    - name: Kill that process ID
      shell: "kill {{ps_output.stdout_lines.0}}"
      when: ps_output.stdout_lines != []
  become: true
  when: action == 'stop-standalone-workers'
# if the worker is running at the named URL and we've been asked to restart it,
# then stop it and start it back up again
- block:
    # get the process ID of the daemon
    - name: Get process ID for daemon
      shell:  "ps aux | grep org.apache.kafka.connect.cli.ConnectStandalone{{worker_grep_str}} | grep -v grep | awk '{print $2}'"
      register: ps_output
    # and kill that process ID
    - name: Kill that process ID and sleep
      shell: "kill {{ps_output.stdout_lines.0}}; sleep 5"
      when: ps_output.stdout_lines != []
    # construct a list of connector properties files
    - name: Get list of names from the connector_list entries
      set_fact:
        connector_names: "{{connector_list | map(attribute='name') | list}}"
    - name: Construct list of corresponding connector properties filenames
      set_fact:
        connector_prop_files: "{{ connector_names | map('regex_replace', '(.*)', '/etc/kafka-connectors/\\1.properties') | list }}"
    # and restart the daemon with the same worker properties used when it was created
    - name: Restart the daemon
      shell: "connect-standalone -daemon /etc/kafka-connectors/{{worker_props_filename}} {{connector_prop_files | join(' ')}}"
      when: ps_output.stdout_lines != []
      become_user: "{{kafka_user}}"
  become: true
  when: action == 'restart-standalone-workers'
